
def wordlistload():
    wordlist2 = ["rt","is","was","are","be","have","had","were","can","said","use","do","will","would","make","like","has","look","write","go","see","could","been","call","am","find","did","get","come","made","may","take","know","live","give","think","say","help","tell","follow","came","want","show","set","put","does","must","ask","went","read","need","move","try","change","play","spell","found","study","learn","should","add","keep","start","thought","saw","turn","might","close","seem","open","begin","got","run","walk","began","grow","took","carry","hear","stop","miss","eat","watch","let","cut","talk","being"]

    wordlist1 = ["rt","is", "was", "are", "be", "have", "had", "were", "can", "said", "use", "do", "will", "would", "make", "like", "has", "look", "write", "go", "see","word", "time", "number", "way", "people", "water", "day", "part", "sound", "work","he", "a", "one", "all", "an", "each","not","of","it","and","I'm"]

    wordlist3 = ["rt","is","was","are","be","have","had","were","can","said","use","do","will","would","make","like","has","look","write","go","see","could","been","call","am","find","did","get","come","made","may","take","know","live","give","think","say","help","tell","follow","came","want","show","set","put","does","must","ask","went","read","need","move","try","change","play","spell","found","study","learn","should","add","keep","start","thought","saw","turn","might","close","seem","open","begin","got","run","walk","began","grow","took","carry","hear","stop","miss","eat","watch","let","cut","talk","being","a","abide","ability","able","about","above","abroad","access","accommodation","accomplish","account","accuracy","accurate","achieve","achievement","acknowledge","acquaintance","acquire","across","act","actual","actually","add","additional","address","advance","advantage","advertise","advertisement","advice","advise","advocate","affair","affect","afford","affordable","afraid","after","afternoon","afterwards","again","against","age","ago","agree","agreement","ahead","aid","aim","albeit","alike","alive","all","allow","allowance","almost","alone","along","already","also","although","always","am","amazing","amend","among","amount","and","anger","angry","ankle","annoy","annoyed","annoying","another","answer","anxious","any","anymore","anyone","anything","anyway","apart","apologize","appeal","appear","appearance","apple","application","apply","appointment","appraisal","appreciate","approach","appropriate","are","area","argue","argument","arise","arm","around","arrange","arrangement","array","arrive","art","as","ashamed","aside","ask","asleep","assert","assertive","assess","assessment","asset","assignment","assume","assumption","at","Ate","attach","attached","attempt","attend","attention","attitude","audience","aunt","available","average","avoid","awake","award","aware","awareness","away","awe","awesome","awful","awkward","bachelor","back","background","bad","bag","bake","balance","bald","ball","ban","band","bank","bar","bare","barely","bargain","bark","base","bass","bat","batch","be","beach","beam","bear","beard","bearing","beat","beautiful","because","become","bed","beef","before","beg","begin","beginning","behave","behavior","behind","being","belief","believe","belong","below","belt","bench","bend","beneath","benefit","beside","besides","best","bet","better","between","beyond","bias","biased","bid","big","bill","bind","binding","bird","birthday","bit","bitch","bite","bitter","blame","blanket","blast","blend","blind","block","blood","blow","blue","blunt","board","boast","boat","body","bold","bolt","bond","book","boost","boot","border","bore","bored","boring","born","borrow","bossy","both","bother","bottom","bounce","bound","boundary","bow","bowl","box","branch","brand","brave","breach","bread","break","breakdown","breakfast","breakthrough","breath","breathe","breed","bridge","brief","bright","bring","broad","broadcast","broke","brother","brown","brush","bucket","budget","bug","build","building","bulk","bully","bump","bunch","bundle","burden","burn","burst","bush","business","bust","busy","but","butt","buy","buzz","by","cabbage","cake","calf","call","called","calm","can","cap","car","care","career","careful","carefully","caring","carry","case","cast","cat","catch","cattle","caught","cause","ceiling","certain","certainly","chain","chair","challenge","challenging","chance","chandelier","change","character","charge","charity","charming","chart","chase","cheap","cheat","check","cheek","cheeky","cheer","cheerful","chest","chicken","chief","child","childhood","chill","chin","choice","choose","chop","church","city","claim","class","clay","clean","clear","clerk","clever","cliff","climb","close","clue","clumsy","cluster","coach","coal","coat","cold","colleague","collect","college","come","comfortable","commit","commitment","committed","common","commute","company","compelling","complain","complaint","complete","compliance","comply","compound","comprehensive","compulsory","computer","concern","concerned","conduct","confidence","confident","consider","consist","consistent","constraint","contact","contain","content","control","convenient","convey","cook","cool","cope","core","correct","cost","costume","couch","cough","could","count","counter","country","couple","course","court","cousin","cover","crack","craft","crap","crash","crawl","crazy","create","creep","creepy","crew","crop","cross","crowd","crowded","crush","cry","cuddle","cue","culture","cup","cupboard","curb","currency","current","currently","curse","custom","customer","cut","cute","daily","damage","damn","damp","dance","dangerous","dare","dark","dash","data","date","daughter","dawn","day","dead","deadline","deal","dear","death","deceive","decide","decision","deck","decline","decrease","deed","deem","deep","deer","default","defeat","definitely","degree","delay","delight","delighted","deliver","delivery","demand","demanding","deny","depict","deploy","depth","deserve","design","desire","desk","despite","dessert","determined","develop","development","device","dictionary","die","different","difficult","dig","dim","dinner","dip","dire","dirty","disappointed","disclosure","discover","discuss","disease","disguise","dish","dismiss","display","distress","ditch","dive","dizzy","do","doctor","does","dog","done","door","doubt","down","draft","drag","drain","draw","drawback","drawer","drawing","drawn","dread","dreadful","dream","dress","drift","drill","drink","drive","drop","drought","drown","dry","due","dull","dumb","dump","during","dust","duty","dwell","dying","each","eager","ear","early","earn","ease","easy","eat","edge","effect","effective","effort","either","elbow","elderly","else","embarrassed","embed","embedded","embrace","emphasize","employee","employer","empty","enable","encompass","encounter","encourage","end","endeavor","endorse","endorsement","endure","engage","engaged","engagement","engine","English","enhance","enjoy","enough","ensure","entail","enter","entire","environment","equipment","equity","escape","especially","essay","establish","even","evening","event","eventually","ever","every","everyone","everything","everywhere","evidence","evil","example","except","exchange","excited","exciting","exercise","expect","expensive","experience","expertise","explain","extent","face","facility","fact","fade","fail","failure","faint","fair","fairly","faith","fake","Fall","family","famous","fan","fancy","far","fare","farewell","farm","fashion","fast","fat","fate","fault","favorite","fear","feasible","feature","fee","feed","feedback","feel","feeling","fell","fellow","felt","fence","fetch","few","field","fierce","fight","figure","file","fill","filthy","finally","find","fine","finish","fire","firm","first","fish","fit","fix","flat","flaw","flawless","flee","flight","flip","flood","floor","flow","flu","fly","focus","fold","follow","following","fond","food","fool","foolish","foot","for","force","forecast","foreign","forget","forgive","form","former","forth","forward","foster","found","frame","framework","freak","free","freight","French","friend","friendly","frightened","fringe","from","front","frown","fuck","fuel","fulfill","full","fun","fund","funny","fur","furniture","further","furthermore","fuss","gain","game","gap","gate","gather","gathering","gauge","gaze","gear","gender","gentle","get","gift","give","glad","glance","glass","glimpse","gloomy","glow","go","goal","goat","going","good","gorgeous","gossip","government","grab","grade","graduate","grant","granted","grasp","grass","grateful","grave","great","greed","greedy","greet","grid","grief","grim","grin","grind","grip","groom","gross","ground","grow","growth","guess","guest","guilty","gut","hail","hair","half","hall","Halloween","hand","handle","handsome","hang","happen","happiness","happy","hard","hardly","harm","harmful","harness","harsh","harvest","has","hat","hate","hatred","have","hazard","he","head","heal","health","healthy","hear","heard","heart","heat","heavy","hedge","heel","height","held","hell","hello","help","helpful","hence","her","here","heritage","hesitate","hi","hidden","hide","high","highlight","hike","hiking","hill","him","hinder","hint","hip","hire","his","history","hit","hoax","hold","hole","holiday","hollow","home","homework","hood","hook","hope","hopefully","host","hot","house","household","how","however","hub","hug","huge","humble","hungry","hunt","hurry","hurt","husband","I","idea","idle","if","ill","illness","impact","implement","imply","important","impressive","improve","improvement","in","include","including","income","increase","indeed","influence","information","injury","inner","input","inquiry","inside","insight","instance","instead","insurance","intend","interest","interested","interesting","interview","into","introduce","invoice","involve","involved","iron","is","issue","IT","item","its","jacket","jam","jar","jealous","jerk","job","join","joint","joke","journey","judge","jump","junk","just","keen","keep","key","kick","kill","kind","knee","knock","know","knowledge","label","lack","ladder","lame","land","landmark","landscape","language","lap","large","last","late","later","latter","laugh","launch","law","lawn","lawyer","lay","layer","layout","lazy","lead","leader","leading","leaf","leak","lean","leap","learn","lease","least","leather","leave","lecture","LED","left","leg","legacy","leisure","lend","length","less","let","letter","level","leverage","liability","lie","life","lift","light","like","likelihood","likely","likewise","limb","line","linger","link","listen","litter","little","live","lively","load","loan","location","lock","log","lonely","long","look","loop","loose","lose","loss","lost","lot","loud","love","lovely","low","lower","loyal","lucky","luggage","lump","lunch","lying","mad","made","main","mainly","mainstream","maintain","major","make","mall","man","manage","management","manager","manner","many","mark","market","married","master","match","mate","matter","May","maybe","ME","meal","mean","meaning","means","meanwhile","measure","meat","media","meet","meeting","melt","merely","merge","mess","messy","middle","might","mild","mind","mine","miss","mistake","mock","money","month","mood","moody","more","moreover","morning","mortgage","most","mostly","mother","mouth","move","much","mud","murder","music","must","my","nail","naive","name","nap","narrow","nasty","nature","naughty","near","nearby","nearly","neat","necessary","neck","need","neglect","neighborhood","neither","nephew","net","network","never","nevertheless","new","news","next","nice","niece","night","nod","noise","noisy","none","nonetheless","not","note","nothing","notice","now","nowadays","null","numb","nurse","nurture","nut","obviously","occur","odd","odds","of","off","offer","offset","often","old","on","once","one","ongoing","only","onto","open","opportunity","opposite","or","order","other","otherwise","ought","our","out","outbreak","outcome","outfit","outgoing","outline","outlook","output","outside","outskirts","outstanding","oven","over","overall","overcome","overlap","overlook","overview","overwhelmed","overwhelming","owe","own","owner","pace","pack","pain","paint","painting","pale","Pan","paper","park","part","partner","party","pass","past","path","patient","pattern","pay","peak","peer","pen","people","perform","performance","perhaps","period","perk","person","pick","picture","pie","piece","pin","pit","pitch","pity","place","plain","plan","plant","plate","play","pleasant","please","pleased","pleasure","pledge","plenty","plot","plug","point","policy","polite","pond","pool","poor","popular","position","post","pot","pound","pour","power","powerful","practice","praise","prefer","present","press","pressure","pretend","pretty","prevent","previous","pride","principal","prior","problem","proceed","process","profit","promise","promote","prompt","prone","proof","proper","properly","property","proposal","prospect","proud","prove","provide","provided","provision","pull","pumpkin","purchase","purpose","purse","pursue","pursuit","push","put","question","queue","quick","quickly","quiet","quit","quite","quote","race","rack","rain","raise","raised","random","range","rank","rare","rash","rate","rather","raw","reach","read","ready","real","realize","really","rear","reason","recall","receipt","receive","recently","reckless","reckon","recognize","recommend","record","recover","refer","reference","reflect","refund","refuse","regard","regarding","regardless","regret","relate","related","relationship","relative","relax","release","relevant","reliability","reliable","relief","relieve","reluctant","rely","remain","remember","remind","remove","render","rent","reply","report","request","require","required","requirement","research","resemble","resort","resource","respect","response","responsible","rest","result","resume","retail","retain","retrieve","return","reveal","revenue","review","reward","rich","rid","ride","right","ring","rise","risk","road","rock","role","roll","roof","room","root","rope","rough","roughly","round","row","rub","rubber","rubbish","rude","rug","rule","run","running","rush","sad","safe","safety","said","sail","sake","same","sample","sand","SAT","save","saw","say","scale","scared","scarf","scary","schedule","scheme","school","scope","score","scrap","scratch","scream","screen","screw","seal","search","season","seat","see","seed","seek","seem","seize","seldom","selfish","sell","send","sense","sensible","sensitive","sentence","serious","serve","service","set","setting","settle","settlement","several","shade","shake","shall","shallow","shame","shape","share","sharp","shed","sheep","sheer","sheet","shelf","shell","shelter","shield","shift","shine","ship","shirt","shit","shoot","shop","shopping","shore","short","shortage","shot","should","shoulder","shout","show","shower","shrink","shut","shy","sibling","sick","side","sigh","sight","sightseeing","sign","significant","silly","similar","sin","since","sincerely","sing","single","sink","sister","sit","size","skill","skin","skinny","skip","skirt","slang","sleep","sleeve","slice","slide","slight","slightly","slim","slip","slope","slow","small","smart","smell","smile","smooth","snap","snow","so","soak","soar","soft","soil","sole","solve","some","somehow","someone","something","son","soon","sore","sorrow","sorry","sort","sought","soul","sound","sour","source","span","spare","spark","speak","speech","spell","spend","spill","spin","split","spoil","spooky","spot","spread","spring","square","squeeze","stack","staff","stage","stain","stake","stakeholder","stall","stand","standard","stare","start","state","statement","status","stay","steady","steak","steal","steam","steel","steep","steer","stem","step","stick","stiff","still","sting","stir","stock","stop","storage","store","storm","story","stove","straight","straightforward","strain","strange","straw","stream","strength","strengthen","stress","stretch","strict","strike","striking","string","strip","strive","stroke","stroll","strong","struck","struggle","stubborn","stuck","student","study","stuff","stumble","stunning","subject","submit","subtle","succeed","success","successful","such","suck","suddenly","suffer","suggest","suit","suitable","summary","summer","sunset","supply","support","sure","surface","surge","surgery","surname","surrender","surround","surrounding","survey","swallow","swap","sway","swear","sweat","sweater","sweep","sweet","swell","swim","swing","switch","swollen","sympathetic","table","tackle","tail","take","talk","talkative","tall","tap","target","task","taste","taught","teach","teacher","team","tear","tease","tell","tend","tender","tense","term","test","than","that","the","their","them","themselves","then","there","thereby","therefore","these","they","thick","thief","thigh","thin","thing","think","this","thorough","thoroughly","those","though","thought","thoughtful","thread","threat","threaten","threshold","thrill","thrive","throat","through","throughout","throw","thrust","thumb","Thursday","thus","tick","tide","tidy","tie","tight","till","time","tin","tiny","tip","tired","to","today","toe","together","toll","tomorrow","tongue","too","tool","top","topic","torn","toss","touch","tough","toward","town","trace","track","trade","trail","train","training","trait","transcription","trap","travel","treat","treatment","tree","trend","trial","trick","tricky","trigger","trim","trip","trouble","trough","TRUE","truly","trunk","trust","trustworthy","truth","try","Tuesday","turkey","turn","turnover","twist","type","ugly","uncle","undefined","under","undergo","underlying","undermine","underneath","understand","understanding","undertake","unfortunately","unique","unless","unlike","unlikely","until","up","update","upgrade","upon","upset","urge","us","use","used","useful","usually","utterly","vacuum","valuable","value","venture","venue","very","vessel","view","visit","void","vtr","wage","waist","wait","wake","walk","wall","wallet","wander","want","ward","warehouse","warm","warn","warning","wash","waste","watch","water","wave","way","we","weak","weakness","wealth","wealthy","wear","weather","Wednesday","week","weekend","weight","weird","welcome","welfare","well","went","were","wet","what","whatever","wheat","wheel","when","whenever","where","whereas","whether","which","while","whilst","whip","WHO","whole","whom","whose","why","wicked","wide","widely","widespread","width","wife","wiggle","wild","will","willing","willingness","win","wind","window","wipe","wire","wisdom","wise","wish","wit","witch","with","withdraw","withdrawal","within","without","witness","witty","woman","wonder","wonderful","wood","word","work","world","worn","worried","worry","worse","worship","worst","worth","worthwhile","worthy","would","wound","wrap","wreck","wrist","write","writing","wrong","yard","year","yell","yet","yield","you","young","your","youth"]

    return wordlist1, wordlist2, wordlist3

wordlist1, wordlist2, wordlist3 = wordlistload()

wordlists = {
    1: wordlist1,
    2: wordlist2,
    3: wordlist3
}

#
# sorry we need those lists for later.. this was the best way.. ':) can't complain it's only 3 lines. right ':)
#
#
# Kali - beta baby
# confusingly written under duress by James Craft Esq circa '24
#
#
# might need to run -  pip install --force-reinstall --no-deps bokeh==2.4.3
# and also need to run pip install streamlit==1.24.0
# at first maybe just give it a go see how it goes?

import subprocess

# List of required packages
required_packages = ['streamlit', 'pandas', 'plotly', 'chardet', 'pathlib', 'matplotlib', 'bokeh']

try:
    import streamlit as st
except NameError:
    print('you dont have streamlit. you need it. use this in your command line - pip install streamlit - kthxbai')

def you_there_guv(package_name):
    try:
        subprocess.check_output(['pip', 'show', package_name])
        return True
    except subprocess.CalledProcessError:
        try:
            subprocess.check_call(['pip', 'install', package_name])
            return True
        except subprocess.CalledProcessError as e:
            print(f"ErrorErrorErrorErrorErrorErrorError: Failed to install {package_name}. Please install it manually using pip. or dont you do you fam.")
            return False

if 'unpackattempt' not in st.session_state:
    for package in required_packages:
        if you_there_guv(package) == False:
            print('ahem it seems we may have an issue here.. it would be wise to attempt (again) an install using pip we have tried but sadly failed')
    st.session_state['unpackattempt'] = 'attempted'



#########import stuff and define some lists.. 
import os
import warnings
import re
import streamlit as st
import pandas as pd
import random
import numpy as np
from datetime import datetime, timedelta
import plotly.express as px 
import base64
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
from bokeh.plotting import figure
#set st key variables
st.set_page_config(
    page_title="KALI",
    page_icon=":dragon_face:",
    layout="wide",
    initial_sidebar_state="collapsed"
)
#search chart function
def barchartplzzits3(master_df, topic_dict, column_list):

    combined_set = set()
    sum_df = pd.DataFrame(columns=('topic_or_keyword','sum_value'))

    for key, value in topic_dict.items():
        combined_set.add(key)
        combined_set.update(value)

    combined_list = list(combined_set)
    master_df = master_df[combined_list]
    #one day.. one day this won't be needed..
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=FutureWarning)
        for col3 in master_df.columns:
            sum_value = master_df[col3].sum()
            topic_or_keyword = col3
            #that day needs the below to work correctly - eh
            #addme = pd.DataFrame(columns=('topic_or_keyword','sum_value'))
            #sum_df = pd.concat([sum_df,addme], ignore_index=True)
            sum_df = sum_df.append({'topic_or_keyword': topic_or_keyword, 'sum_value': sum_value}, ignore_index=True)

    return master_df, sum_df
#used when we load data
@st.cache_data
def get_string_columns(df):
    string_columns = []
    for col in df.columns:
        if df[col].dtype == 'object':  # Check if the dtype is 'object' (usually strings)
            if df[col].dtype != 'bool':
                    string_columns.append(col)
    return string_columns
#creates counts.. i think
def create_keyword_columns(master_df, topic_dict, column_list):
    # Iterate through each topic and its keywords
    for topic, keywords in topic_dict.items():
        # Iterate through each column in column_list and check if the keyword is present
        for col2 in column_list:
            # Check if the column exists in the DataFrame
            if col2 in master_df.columns:
                # Check if any keyword is present in the column and increment the counts
                for keyword in keywords:
                    if col2 == topic:
                        master_df[topic] += master_df[col2].str.contains(fr'{keyword}', case=False, regex=True).astype(int)
                    else:
                        master_df[keyword] = master_df[col2].str.contains(fr'{keyword}', case=False, regex=True).astype(int)
                        if topic not in master_df.columns:
                            master_df[topic] = 0
                        master_df[topic] += master_df[keyword]
                        master_df[topic] = (master_df[topic] > 0).astype(int)
    return master_df
#search
def Search_Data(master_df, topic_dict, column_list):
    columns_to_drop = [col for col in master_df.columns if col not in column_list]
    master_df.drop(columns=columns_to_drop, inplace=True)
    master_df = create_keyword_columns(master_df, topic_dict, column_list)
    st.write('Topics and Keywords flagged')
    st.write(master_df.head(50))
    
    return master_df
#download search data- name should be better
def Download_Data(sumout, topic_dict, column_list):
    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
    filename = f'KALI_SEARCH_{timestamp}.csv'
    sumout.to_csv(filename, index=False)
    st.write('saved file in your user root as '+filename)

#graph search data
def Graph_Data(master_df, topic_dict, column_list):
    #calls another function.. k ik this isn't amazingly pythonic. dealwithit.jpeg
    #   you fool.
    #       dosen't return anything.. not sure if it needs to.. 
    master_df, sum_df = barchartplzzits3(master_df, topic_dict, column_list)
    #sum_df = sum_df.sort_values(by='sum_value')
    sum_df_sorted = sum_df.sort_values(by='sum_value', ascending=False)
    lenght_of_mdf = len(master_df)
    Total_flagged = sum_df['sum_value'].sum()
    sum_df_sorted['percent of entries'] = (100. * sum_df_sorted['sum_value']/lenght_of_mdf)
    sum_df_sorted['percent of total flagged'] = (100. * sum_df_sorted['sum_value']/Total_flagged)
                                                                                                                                                                        #on the way out
                                                                                                                                                                        #graph_1_sum_df_sorted = pd.DataFrame(columns=('topic_or_keyword','percent of total flagged'))
                                                                                                                                                                        #graph_2_sum_df_sorted = pd.DataFrame(columns=('topic_or_keyword','percent of entries'))
                                                                                                                                                                        #graph_1_sum_df_sorted = sum_df_sorted['topic_or_keyword','percent of total flagged'].copy()
                                                                                                                                                                        #graph_2_sum_df_sorted = sum_df_sorted['topic_or_keyword','percent of entries'].copy()
    
    
    #create new dfs with just the topic keyword and %'s
    graph_1_sum_df_sorted = pd.DataFrame(data=sum_df_sorted, columns=['topic_or_keyword','percent of total flagged'])
    graph_2_sum_df_sorted = pd.DataFrame(data=sum_df_sorted, columns=['topic_or_keyword','percent of entries'])
    graph_1_sum_df_sorted = graph_1_sum_df_sorted.reset_index(drop=True) 
    graph_2_sum_df_sorted = graph_2_sum_df_sorted.reset_index(drop=True)
    
    #use apply function to format floats to percentages
    sum_df_sorted['percent of entries'] = sum_df_sorted['percent of entries'].apply(lambda x: f'{x:.2f}%')
    sum_df_sorted['percent of total flagged'] = sum_df_sorted['percent of total flagged'].apply(lambda x: f'{x:.2f}%')

    

    #some popups about the search
    #st.write('Total flags found: ' + str(sum_df_sorted['sum_value'].sum())+'Total rows used: ' + str(len(master_df)) + 'You have matched '  )
    #st.write('Total flags found: ' + str(Total_flagged) + 'Total rows used: ' + str(lenght_of_mdf) + 'You have matched ' + int(Total_flagged/lenght_of_mdf)*100 + r'% of rows entered' )
    st.write('Total flags found: ' + str(Total_flagged) + ' Total rows: ' + str(lenght_of_mdf) + ' Your match rate is ' + str(int((Total_flagged/lenght_of_mdf)*100)) + '%' )
    st.subheader('Results by topic or keyword')
    st.write(sum_df_sorted)

    flaggedcol, totalcol = st.columns(2)
    
    with flaggedcol:
        st.bar_chart(data=graph_1_sum_df_sorted, x='topic_or_keyword', y='percent of total flagged')
    
    with totalcol:
        st.bar_chart(data=graph_2_sum_df_sorted, x='topic_or_keyword', y='percent of entries')

    return sum_df_sorted
#frequency 
def create_frequency_list(master_df, column_list):
    word_counts = {}  # Dictionary to store word counts
    for col in column_list:
        if col in master_df.columns:
            # Iterate through each row in the column and count word frequencies
            for value in master_df[col]:
                words = str(value).split()  # Split the value into words
                for word in words:
                    word = word.lower()  # Convert to lowercase for case insensitivity
                    word_counts[word] = word_counts.get(word, 0) + 1  # Update word count

    # Create a DataFrame from the word counts dict
    word_df = pd.DataFrame({'word': list(word_counts.keys()), 'count': list(word_counts.values())})
    word_df = word_df.sort_values(by='count', ascending=False)
    word_df = word_df.sort_values("count", ascending=False)
    word_df.rename(columns={"word":"Word","count":"Count"}, inplace=True)
    word_df = word_df[~(word_df['Word'] == 'null')]

    return word_df
#blacklist 
def removeblacklist4(word_df,word_num):
    #the only one with any actual thought required
                    #goal
        #word_num->wordlist[word_num]->wordlist
        #blacklist_main=wordlist
        #blacklist_main+=userdefined(ssblist)
        #greylist=blacklist_main-whitelist
        #cleanlist=word_df-greylist
    #
    #build out key list objects
    wordlist = wordlists[word_num]
    blacklist_main = []
    greylist = []
    # Extend blacklist_main with wordlist
    blacklist_main = wordlist 
    #create err easier list for me to fox wit. ':)
    ssblist = [i for i in st.session_state['unique_key_blacklist'].split(',')]
    #for each of the user defined words if its aleady in our list do nothing if it's not add it. ad infinum.
    for i in ssblist:
        if i not in blacklist_main:
            blacklist_main.append(i)
    #for each of the user defined white list words if it's in our blacklist.. lets remove it.. created another lazy shortcut here..sorry
    sswlist = [i for i in st.session_state['unique_key_whitelist'].split(',')]
    for i in sswlist:
        if i in blacklist_main:
            blacklist_main.remove(i)
    
    #unpythonicaf.
    greylist = blacklist_main
    #prep a new clean df to load into
    cleanlist = pd.DataFrame(columns=['Word', 'Count'])

    for index, row in word_df.iterrows():
        if row['Word'] not in greylist:
            cleanlist = pd.concat([cleanlist, pd.DataFrame({'Word': [row['Word']], 'Count': [row['Count']]})], ignore_index=True)
    return cleanlist
#another frequncy 
def wordfreqs(word_df):
    clean1 = removeblacklist4(word_df,1)
    clean2 = removeblacklist4(word_df,2)
    clean3 = removeblacklist4(word_df,3)
    return clean1, clean2, clean3
#data upload 
def uploadedfilesload(uploaded_files):
    dfs = []
    errors = []

    for file in uploaded_files:
        file_name = file.name
        try:
            # Attempt to read CSV file
            df = pd.read_csv(file, header=0, encoding='utf-16-le', sep='\t')
        except pd.errors.ParserError:
            try:
                # Attempt to read Excel file
                df = pd.read_excel(file)
            except Exception as e:
                # Handle other file formats or errors as needed
                errors.append(f"Error reading file {file_name}: {str(e)}")
                continue  # Skip to the next file

        # Drop numerical columns
        for col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                df.drop(col, axis=1, inplace=True)

        # Drop bool columns # could pop a is str check in here to be super careful
        for col in df.columns:
            if pd.api.types.is_bool_dtype(df[col]):
                df.drop(col, axis=1, inplace=True)

        # Fill missing values and add query column
        df.fillna('null', inplace=True)
        df.dropna(inplace=True)
        df['query'] = file_name

        dfs.append(df)
        
        for i in df.columns:
            df[i] = df[i].astype(str)

    # Attempt to serialize DataFrame to Arrow table - just checking we can actually fox wit dis
    try:
        master_df.to_feather("temp.feather")  # Save to temporary file
    except Exception as e:
        errors.append(f"do better james. this is shocking. here's the exception: {str(e)}")

    if errors:
        print("Errors encountered during data loading:")
        for error in errors:
            print(error)

    # special_characters = [
    #     '!', '$', '%', '^', '&', '*', '(', ')', '_', '-', '+', '=', '[', ']', '{', '}', '|',
    #     ';', ':', "'", '"', '.', '/', '<', '>', '?', '~', '`', '^', '©', '®', '™'
    # ]

    # for i in special_characters:
    #     master_df = master_df.replace(re.escape(i), '', regex=True)
    # Sample special characters


    #dont ask me how the below works while the above does not. frankly i'm just happy there's no errors.
    special_characters = "!$%^&*()_-+=[]{}|;:'\"./<>?~`^©®™"

    # Create a translation dictionary mapping each special character to an empty string
    translation_dict = {ord(char): None for char in special_characters}

    # Apply the translation to each element in the Series
    master_df = master_df.apply(lambda x: x.translate(translation_dict) if isinstance(x, str) else x)

    return master_df
#sets up search
def setup_search():
    st.markdown("""
            <h2>🔍 Search Data <h2>
            """, unsafe_allow_html=True)
    st.subheader('Topic Groups')
    st.text('Build out the topic groups you are searching for and the keywords that comprise those groups. Make sure to use commas to seperate your values.') 
    st.text('Please note I can take a moment to populate the keyword input fields for you - any changes to the topic groups after you have entered keywords WILL cause me to freak out and lose your query. Also erm im case sensitive so play gently..')
    st.text('Be as exact and literal as you can with me - I love lots of keywords - I wont double count your flags :)')
    topic_list = st.text_area('Topics', 'topic-eu,topic-carbon').split(',')
    st.subheader('Made of These Keywords')
    st.text(r' .* Now Regular Expression Friendly! .* ')
    # to a degree ' :) #builds the topic_lists
    for topic in topic_list:
        topic_dict[topic] = st.text_area(f'Keywords for {topic}', 'europe,brexit').split(',')
    return topic_dict, topic_list

#########LETSGETLIT.STREAMLIT

# title
#st.title('KALI')
st.markdown("""
# :dragon: KALI 
""")
st.text(' ')
st.text('Keyword Analysis and Language Insights')
st.text(' ')
st.markdown("""
🥜 *"God gives us the nuts, but he does not crack them"* 🥜
""", unsafe_allow_html=True)

#init topic dict
topic_dict = {}
st.text(' ')
# set your files/file using file_uploader
uploaded_files = st.file_uploader("Choose CSV files", type=["csv"], accept_multiple_files=True)
dfs = []

#ssmaster needs to be initalised if it isn't and if it is lets set it to 1
if 'ssmaster' not in st.session_state:
    st.session_state['ssmaster'] = 1

#main operational bits start from here really
if uploaded_files:
    master_df = uploadedfilesload(uploaded_files)
    string_columns = get_string_columns(master_df)
    st.write(f'The data has been combined, below is a small chunk. In total, {len(master_df)}'+' rows were ingested.')
    total_rows = len(master_df)
    st.write(master_df.head(50))
    st.markdown("""
                <h2>🏛️ Use which Columns?<h2>
                """, unsafe_allow_html=True)
    column_list = st.multiselect(r'Columns to Search in..', string_columns)
    if len(column_list)!=0:
        st.session_state['ssmaster'] = 2

if 'page' not in st.session_state:
    st.session_state['page'] = 'none'

st.markdown("""
            <h2>🤖 Use which Function?<h2>
            """, unsafe_allow_html=True)

st.write("Search can calculate counts across topics for multiple user-defined keywords.")

if st.button('Search') and st.session_state['ssmaster'] == 2:
    st.session_state['page'] = 'search'

st.write("Frequency creates word frequency lists with customisable white and blacklists to remove unwanted terms.")
st.write("Frequency can be useful if you don't know what to search for or want to see similar terms quickly")
if st.button('Frequency') and st.session_state['ssmaster'] == 2:
    st.session_state['page'] = 'freq'

if st.session_state['page'] == 'search' and st.session_state['ssmaster'] == 2:
    topic_dict, topic_list = setup_search()

    if st.session_state['ssmaster'] == 2 and st.session_state['page'] == 'search':
        if st.button('Run Search') and st.session_state['ssmaster'] == 2 and st.session_state['page'] == 'search':
            master_df = Search_Data(master_df, topic_dict, column_list)
            sumout = Graph_Data(master_df, topic_dict, column_list)

 
elif st.session_state['page'] == 'freq' and st.session_state['ssmaster'] == 2:
    word_df = create_frequency_list(master_df, column_list)
    st.markdown("""
            <h2>🧮 Word Frequency Counts<h2>
            """, unsafe_allow_html=True)
    cola, colb, colc = st.columns(3)
    with cola:
        whitelist_input = st.text_area('Enter your Whitelist Keywords here',
                                        'example,entry',
                                        key='unique_key_whitelist')
    with colb:
        blacklist_input = st.text_area('Enter your Blacklist Keywords here',
                                        'here,andhere',
                                        key='unique_key_blacklist')
    with colc:
        clean1,clean2,clean3 = wordfreqs(word_df)
        freqnum = st.slider(label='Number of entries to display.',
                            value=250,
                            min_value=len(clean3),
                            max_value=len(clean1),
                            key='freq_slider'
                        )
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.write('This is the vanilla list')
        st.write(word_df.head(freqnum)) 
    with col2:
        st.write("lil filter")
        st.write(clean1.head(freqnum))
    with col3:
        st.write("big filter") 
        st.write(clean2.head(freqnum))
    with col4:
        st.write("el filtero primero") 
        st.write(clean3.head(freqnum))
     
